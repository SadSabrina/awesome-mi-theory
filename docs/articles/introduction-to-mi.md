# Introduction to Mechanistic interpretability

### **A Bit of History**

Mechanistic Interpretability (MI) is a relatively new paradigm within the field of Explainable AI. However, its core idea is not new â€” it can be found in discussions within neuroscience [[1](https://www.sciencedirect.com/science/article/abs/pii/S1001074215000200), [2](https://link.springer.com/chapter/10.1007/978-3-319-22084-0_2), [3](https://www.sciencedirect.com/science/article/pii/S0888754314001773)] and cognitive science.  

![MI_intro_IMG](https://ucarecdn.com/d2c11722-9a16-42d4-9145-ee8f52942cf9/-/crop/1132x369/0,3/-/preview/)
*Philosophy of Cognitive Science in the Age of Deep Learning, May 2024*  

When applied to models, the concept of mechanistic interpretability can be used not only for neural networks but also for other algorithms, such as Support Vector Machines (SVMs) [4]. This highlights that mechanistic interpretability is not a unique discovery, but rather an intriguing and multifaceted idea that has proven useful in analyzing complex models, including deep neural networks.  
